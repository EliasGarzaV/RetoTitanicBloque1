{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from cmath import nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../Data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list(train['Ticket'].str.split(\" \", expand = True)[0])\n",
    "letras = []\n",
    "for i in d:\n",
    "    if i[0].isalpha()==True:\n",
    "        letras.append(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"s\".isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass   Age  SibSp  Parch     Fare  Sex_female  Sex_male  \\\n",
       "0           0       3  22.0      1      0   7.2500           0         1   \n",
       "1           1       1  38.0      1      0  71.2833           1         0   \n",
       "2           1       3  26.0      0      0   7.9250           1         0   \n",
       "3           1       1  35.0      1      0  53.1000           1         0   \n",
       "4           0       3  35.0      0      0   8.0500           0         1   \n",
       "..        ...     ...   ...    ...    ...      ...         ...       ...   \n",
       "885         0       3  39.0      0      5  29.1250           1         0   \n",
       "886         0       2  27.0      0      0  13.0000           0         1   \n",
       "887         1       1  19.0      0      0  30.0000           1         0   \n",
       "889         1       1  26.0      0      0  30.0000           0         1   \n",
       "890         0       3  32.0      0      0   7.7500           0         1   \n",
       "\n",
       "     Embarked_C  Embarked_Q  Embarked_S  \n",
       "0             0           0           1  \n",
       "1             1           0           0  \n",
       "2             0           0           1  \n",
       "3             0           0           1  \n",
       "4             0           0           1  \n",
       "..          ...         ...         ...  \n",
       "885           0           1           0  \n",
       "886           0           0           1  \n",
       "887           0           0           1  \n",
       "889           1           0           0  \n",
       "890           0           1           0  \n",
       "\n",
       "[714 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train = train.dropna(subset='Age')\n",
    "del train['PassengerId']\n",
    "del train['Name']\n",
    "del train['Ticket']\n",
    "del train['Cabin']\n",
    "dummies = pd.get_dummies(train[[\"Sex\", \"Embarked\"]])\n",
    "del train['Sex']\n",
    "del train['Embarked']\n",
    "train = pd.concat([train, dummies], axis=1)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.boxplot(figsize=(18,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('Survived', axis=1)\n",
    "Y_train = train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>3</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass   Age  SibSp  Parch      Fare  Sex_female  Sex_male  Embarked_C  \\\n",
       "0         3  34.5      0      0    7.8292           0         1           0   \n",
       "1         3  47.0      1      0    7.0000           1         0           0   \n",
       "2         2  62.0      0      0    9.6875           0         1           0   \n",
       "3         3  27.0      0      0    8.6625           0         1           0   \n",
       "4         3  22.0      1      1   12.2875           1         0           0   \n",
       "..      ...   ...    ...    ...       ...         ...       ...         ...   \n",
       "413       3  26.0      0      0    8.0500           0         1           0   \n",
       "414       1  39.0      0      0  108.9000           1         0           1   \n",
       "415       3  38.5      0      0    7.2500           0         1           0   \n",
       "416       3  26.0      0      0    8.0500           0         1           0   \n",
       "417       3  26.0      1      1   22.3583           0         1           1   \n",
       "\n",
       "     Embarked_Q  Embarked_S  \n",
       "0             1           0  \n",
       "1             0           1  \n",
       "2             1           0  \n",
       "3             0           1  \n",
       "4             0           1  \n",
       "..          ...         ...  \n",
       "413           0           1  \n",
       "414           0           0  \n",
       "415           0           1  \n",
       "416           0           1  \n",
       "417           0           0  \n",
       "\n",
       "[418 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"../Data/test.csv\")\n",
    "test\n",
    "test['Age'] = test['Age'].replace(np.nan, 26, regex=True)\n",
    "del test['PassengerId']\n",
    "del test['Name']\n",
    "del test['Ticket']\n",
    "del test['Cabin']\n",
    "dummies = pd.get_dummies(test[[\"Sex\", \"Embarked\"]])\n",
    "del test['Sex']\n",
    "del test['Embarked']\n",
    "test['Fare'] = test['Fare'].replace(np.nan, 35.62, regex=True)\n",
    "test = pd.concat([test, dummies], axis=1)\n",
    "X_test = test\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.1):\n",
    "        # initialize the list of weights matrices, then store the\n",
    "        # network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    " \n",
    "        # start looping from the index of the first layer but\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
    "            self.W.append(w / np.sqrt(layers[i]))\n",
    " \n",
    "        # the last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
    "        self.W.append(w / np.sqrt(layers[-2]))\n",
    " \n",
    "    def __repr__(self):\n",
    "        # construct and return a string that represents the network\n",
    "        # architecture \n",
    "        return \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.layers))\n",
    " \n",
    "    def sigmoid(self, x):\n",
    "        # compute and return the sigmoid activation value for a\n",
    "        # given input value\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    " \n",
    "    def sigmoid_deriv(self, x):\n",
    "        # compute the derivative of the sigmoid function ASSUMING\n",
    "        # that `x` has already been passed through the `sigmoid`\n",
    "        # function\n",
    "        return x * (1 - x)\n",
    " \n",
    "    def fit(self, X, y, epochs=1000, displayUpdate=100):\n",
    "        # insert a column of 1's as the last entry in the feature\n",
    "        # matrix -- this little trick allows us to treat the bias\n",
    "        # as a trainable parameter *within* the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    " \n",
    "        # loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point and train\n",
    "            # our network on it\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    " \n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch + 1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={:.7f}\".format(epoch + 1, loss))\n",
    " \n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer\n",
    "        # as our data point flows through the network; the first\n",
    "        # activation is a special case -- it's just the input\n",
    "        # feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    " \n",
    "        # FEEDFORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # take the dot-product between the activation and\n",
    "            # the weight matrix -- this is called the \"net input\"\n",
    "            # to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    " \n",
    "            # computing the \"net output\" is simply applying our\n",
    "            # nonlinear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    " \n",
    "            # once we have the net output, add it to our list of\n",
    "            # activations\n",
    "            A.append(out)\n",
    " \n",
    "        # BACKPROPAGATION\n",
    "        # the first phase of backpropagation is to compute the\n",
    "        # difference between our *prediction* (the final activation\n",
    "        # in the activations list) and the true target value\n",
    "        error = A[-1] - y\n",
    " \n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas `D`; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        D = [error * self.sigmoid_deriv(A[-1])]\n",
    " \n",
    "        # once you understand the chain rule it becomes super easy\n",
    "        # to implement with a `for` loop -- simply loop over the\n",
    "        # layers in reverse order (ignoring the last two since we\n",
    "        # already have taken them into account)\n",
    "        for layer in np.arange(len(A) - 2, 0, -1):\n",
    "            # the delta for the current layer is equal to the delta\n",
    "            # of the *previous layer* dotted with the weight matrix\n",
    "            # of the current layer, followed by multiplying the delta\n",
    "            # by the derivative of the nonlinear activation function\n",
    "            # for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta * self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    " \n",
    "        # since we looped over our layers in reverse order we need to\n",
    "        # reverse the deltas\n",
    "        D = D[::-1]\n",
    " \n",
    "        # WEIGHT UPDATE PHASE\n",
    "        # loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # update our weights by taking the dot product of\n",
    "            # the layer activations with their respective deltas,\n",
    "            # then multiplying this value by some small learning\n",
    "            # rate and adding to our weight matrix -- this is where\n",
    "            # the actual \"learning\" takes place\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    " \n",
    "    def predict(self, X, addBias=True):\n",
    "        # initialize the output prediction as the input features -- this\n",
    "        # value will be (forward) propagated through the network to\n",
    "        # obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    " \n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1's as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    " \n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # computing the output prediction is as simple as taking\n",
    "            # the dot product between the current activation value `p`\n",
    "            # and the weight matrix associated with the current layer,\n",
    "            # then passing this value through a nonlinear activation\n",
    "            # function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    " \n",
    "        # return the predicted value\n",
    "        return p\n",
    " \n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    " \n",
    "        # return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([10,5,6,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=62910.5420210\n",
      "[INFO] epoch=100, loss=73603.0512307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_22364\\1527977702.py:31: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=200, loss=75985.6481373\n",
      "[INFO] epoch=300, loss=71749.4193051\n",
      "[INFO] epoch=400, loss=67426.7994137\n",
      "[INFO] epoch=500, loss=70358.8840855\n",
      "[INFO] epoch=600, loss=70759.5708975\n",
      "[INFO] epoch=700, loss=71091.6140487\n",
      "[INFO] epoch=800, loss=72393.8279014\n",
      "[INFO] epoch=900, loss=69868.8949412\n",
      "[INFO] epoch=1000, loss=70269.8201292\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train,Y_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elian\\AppData\\Local\\Temp\\ipykernel_22364\\1527977702.py:31: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nn.predict(X_test)\n",
    "y_pred = y_pred.round().tolist()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    predictions.append(y_pred[i][0])\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_o = pd.read_csv(\"test.csv\")\n",
    "data = {'PassengerId' : test_o['PassengerId'],\n",
    "'Survived' : predictions}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('gender_submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9358f01635aaa8aae510f26f7ad6d28c2c17edde979f59b7532c3bf17de159b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
